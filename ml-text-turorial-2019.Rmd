---
title: "Introduction to Supervised Machine Learning with text data"
author: "Vlad Achimescu"
output: html_notebook
---

## Setup

```{r results='hide', message=FALSE, warning=FALSE}
library(quanteda)
library(caret)
library(glmnet)
library(Matrix)
library(readxl)
library(dplyr)
library(lubridate)
library(readr)
```


## Read data

NOTE: must clean and retain only useful metadata

```{r}
REDDIT = read_rds("data/RedditExeter.rds")
REDDIT
```

```{r explore}
summary(REDDIT)
```

List of Trolls
```{r}
trolls = factor(REDDIT$author[REDDIT$is_troll=="TROLL"])
print(paste("Number of identified trolls: ", length(trolls)))
table(trolls)
```

List of subreddits
```{r}
table(REDDIT$subreddit, REDDIT$is_troll)
```


To reduce loading times, we only select data from one month of 2018 (may)

```{r summarise}
REDDIT_may = REDDIT[REDDIT$month=="May",]
table(REDDIT_may$is_troll)
```


### Text processing

Separate into: corpus / tokenization / pre-processing / stemming / n-grams / trim / weight

```{r}
R_corpus = corpus(REDDIT_may[,c("id","title",
                                "hour","num_comments_log","logscore",
                                "is_self","self_text","is_troll")], 
                  docid_field="id", 
                  text_field="title")
names(docvars(R_corpus)) = paste0("META_",names(docvars(R_corpus)))
R_corpus
```

Tokenize

```{r}
tokens(R_corpus, what = c("sentence"))[1:2]
tokens(R_corpus, what = c("word"))[1:2]
tokens(R_corpus, what = c("character"))[1:2]
```

```{r tokenize words}
R_tokens = tokens(R_corpus, what = c("word"))
R_tokens[1:5]
print("Token lengths: "); cat("\n\n")
print(sapply(R_tokens[1:5], length))
```

Remove unnecessary tokens

```{r}
R_tokens_cleaned = R_tokens %>%
                   tokens_tolower() %>%
                   tokens(R_tokens, remove_numbers = TRUE, 
                        remove_punct = TRUE, 
                        remove_symbols = TRUE, 
                        remove_separators = TRUE,
                        remove_hyphens = FALSE) %>%
                   tokens_remove(stopwords("en"))
                 
R_tokens_cleaned[1:5]
print("Token lengths: "); cat("\n")
print(sapply(R_tokens[1:5], length))
```

Stem

```{r}
R_tokens_stemmed = R_tokens_cleaned %>% tokens_wordstem(language = "en")
R_tokens_stemmed[1:5]
```

N-grams
```{r}
unigrams = unigrams; 
    cat("\nUnigrams :"); print(unigrams)
bigrams = unigrams %>% tokens_ngrams(n=2); 
    cat("\nBigrams :"); print(bigrams)
trigrams = unigrams %>% tokens_ngrams(n=3); 
    cat("\nTrigrams :"); print(bigrams)
skipgrams1 = unigrams %>% tokens_ngrams(n=3, skip = c(0,1))
    cat("\nTrigrams(including 1-skip) :"); print(skipgrams1)

### Add ngrams to tokens 
cat("\n\nAdding nGrams:\n")
print("- keeping onegrams")
unigrams %>% tokens_compound(phrase("barack obama"), join = TRUE)
print("- dropping onegrams")
unigrams %>% tokens_compound(phrase("barack obama"), join = FALSE)
```


Create document-feature-matrix (DFM)

```{r}
R_DFM = dfm(R_tokens_stemmed) 
cat("\nInitial sparsity: \n")
R_DFM
```

Trim DFM
```{r}
### Trim options
cat("\nFinal sparsity: \n")

dfm_trim(R_DFM, min_termfreq =  10)
dfm_trim(R_DFM, min_termfreq = 100)
dfm_trim(R_DFM, min_docfreq =   10)
dfm_trim(R_DFM, min_docfreq =  100)

###
cat("\nFinal sparsity: \n")
R_DFM_trim = dfm_trim(R_DFM, min_docfreq = 0.005,
                             max_docfreq = 0.900,
                             docfreq_type = "prop")
R_DFM_trim
```

Clean and display
```{r}
### clean: replace feature names that are not valid variable names
R_DFM_cleaned = dfm_replace(R_DFM_trim, 
                            featnames(R_DFM_trim), 
                            make.names(featnames(R_DFM_trim), unique = TRUE))
R_DFM_cleaned
R_DFM_cleaned[1:10,1:10]
```

Working with DFM
```{r}
cat("First ten document names:\n")
  docnames(R_DFM_cleaned)[1:10]
cat("First ten feature names:\n")
  featnames(R_DFM_cleaned)[1:10]
cat("Names of features in metadata:\n")
  names(docvars(R_DFM_cleaned))
```


Explore dfm
```{r}
topfeatures(R_DFM_cleaned, groups = docvars(R_DFM_cleaned)$META_is_troll)
```

Weight DFM
```{r}
R_DFM_prop = R_DFM_cleaned %>% dfm_weight("prop")
R_DFM_TFIDF = R_DFM_cleaned %>% dfm_tfidf(scheme_tf = "count")
R_DFM_TFIDF_prop = R_DFM_cleaned %>% dfm_tfidf(scheme_tf="prop")
cat("Compare values weighted/unweighted:\n")
R_DFM_cleaned[1:5,1:5]
R_DFM_prop[1:5,1:5]
R_DFM_TFIDF[1:5,1:5]
R_DFM_TFIDF_prop[1:5,1:5]
```

ALL-IN-ONE
```{r Final dataset}
R_corpus
R_DFM_final =tokens(R_corpus, what = c("word")) %>% 
             tokens_tolower() %>%
             tokens(remove_numbers = TRUE, 
                                  remove_punct = TRUE, 
                                  remove_symbols = TRUE, 
                                  remove_separators = TRUE,
                                  remove_hyphens = FALSE) %>%
             tokens_remove(stopwords("en")) %>% 
             tokens_wordstem(language = "en") %>% 
    dfm() %>% 
    dfm_trim(min_docfreq = 0.005,
             max_docfreq = 0.900,
             docfreq_type = "prop") %>% 
    dfm_replace(featnames(R_DFM_trim), 
                make.names(featnames(R_DFM_trim), unique = TRUE)) %>% 
    dfm_tfidf(scheme_tf = "count")

R_DFM_final = R_DFM_TFIDF
```

## Machine Learning

Separate training set and test set

```{r}
TEXT = convert(R_DFM_final, to = "data.frame")[,-1]
METADATA = docvars(R_DFM_final)
X = cbind(TEXT , METADATA) 
X = model.matrix(META_is_troll~., X)
View(X[1:20,])
```


```{r}
set.seed(735)
train <- sample(1:nrow(R_DFM_final), 0.7*nrow(R_DFM_final))
length(train)
###
reddit_train_X <- X[train,]
  reddit_test_X <- X[-train,]
reddit_train_Y = docvars(R_DFM_final)$META_is_troll[train]
  reddit_test_Y = docvars(R_DFM_final)$META_is_troll[-train] 
```


### Lasso

Build pipeline in caret

```{r}
library(caret)
set.seed(773)

### Setting values for hyperparameters
tuneGrid_glm = expand.grid(alpha = c(1), 
                           lambda = 10^seq(-4, -1, length = 20) )

### Specifying cross-validation method
ctrl <- trainControl(method = "cv", number = 5,
                     classProbs = TRUE, 
                     summaryFunction = prSummary)
tuneGrid_glm
```

Training the model

```{r}
### 
train_lasso <- train(x = reddit_train_X, y = reddit_train_Y, 
                     method = "glmnet",
                     metric = "F",
                     tuneGrid = tuneGrid_glm,
                     trControl = ctrl)
train_lasso
```


```{r}
#plot(m2, label=T, xvar = "lambda")
#train_lasso
plot(train_lasso, label=T, xvar = "lambda")
```

Get final model
```{r}
finalModel_lasso = train_lasso$finalModel
bestTune_lasso  = train_lasso$bestTune
bestTune_lasso
```


Plotting the coefficient paths.

```{r}
plot(finalModel_lasso, label=T, xvar = "lambda")
```


Check coefficients
```{r}
coef_lasso = coef(finalModel_lasso, bestTune_lasso$lambda) %>% as.vector
  names(coef_lasso) = colnames(reddit_train_X)
print("Features: TROLL")
  coef_lasso %>% sort %>% rev %>%  head(30)
cat("\n")
print("Features: NOT troll")
  coef_lasso %>% sort %>% head(30)
```


### Prediction in test set

Confusion matrix for cross-validated

```{r}
p_lasso <- predict(finalModel_lasso, 
                   s = bestTune_lasso$lambda, 
                   newx = reddit_test_X, type = "class") %>% factor
cfm = confusionMatrix(p_lasso, 
                      reddit_test_Y, 
                      positive = "TROLL", 
                      mode = "prec_recall")
cfm
```


### Random Forests

Train and cross-validate to get optimally tuned hyperparameters

```{r}
tuneGrid_rf = expand.grid(mtry = c(10, 15, 20, 25))
ctrl_rf <- trainControl(method = "cv", number = 5, classProbs = TRUE, 
                        savePredictions = TRUE, summaryFunction = twoClassSummary)
  
train_rf <- train(x = reddit_train_X, y = reddit_train_Y, 
                      method = "rf",
                      tuneGrid = tuneGrid_rf,
                      metric = "ROC",
                      trControl = ctrl_rf)
train_rf
```

Predict on test set

```{r}
c_rf <- predict(train_rf, newdata = reddit_test_X)
r_rf <- predict(train_rf, newdata = reddit_test_X, type = "prob")
confusionMatrix(c_rf, factor(reddit_test_Y), positive = "TROLL", mode = "prec_recall")
confusionMatrix(c_rf, factor(reddit_test_Y), positive = "TROLL")
```

Calculate AUC
```{r}
dat = data.frame(obs =  relevel(reddit_test_Y, 2),
               pred = relevel(c_rf,2),
               TROLL = r_rf[,2])
twoClassSummary(dat, lev = c("TROLL","not_troll"))
prSummary(dat, lev = c("TROLL","not_troll"))
```

Feature importance

```{r}
varImp(train_rf)
```

Partial dependence plots

```{r, fig.align="center"}
library(pdp)
pdp1 <- partial(train_rf, pred.var = "META_hour", trim.outliers = T, grid.resolution=24, 
                type = "classification", prob=TRUE, which.class = "TROLL")
plotPartial(pdp1, train = reddit_train_X, rug = TRUE, alpha = 0.3)
```


## Create an ensemble - 

```{r}
ctrl <- trainControl(method = "cv", number = 5,
                     classProbs = TRUE, 
                     summaryFunction = prSummary,
                     savePredictions = TRUE)

train_lasso1 <- train(x = reddit_train_X, y = reddit_train_Y, 
                     method = "glmnet",
                     metric = "F",
                     tuneGrid = tuneGrid_glm,
                     trControl = ctrl)
train_lasso2 <- train(x = reddit_train_X[,1:218], y = reddit_train_Y, 
                     method = "glmnet",
                     metric = "F",
                     tuneGrid = tuneGrid_glm,
                     trControl = ctrl)


model.list <- list( train_lasso1, train_lasso2)
class(model.list) <- "caretList"
caretEnsemble(model.list, method='glmnet')
```


## Predict on new test set

```{r}

```



# Work on your own datasets

### Text processing
```{r}
### read data frame

### define corpus
OWNcorpus_own = corpus()

### create dfm

OWNDFM_final =tokens(OWNcorpus, what = c("word")) %>% 
  tokens_tolower() %>%
  tokens(remove_numbers = TRUE, 
         remove_punct = TRUE, 
         remove_symbols = TRUE, 
         remove_separators = TRUE,
         remove_hyphens = FALSE) %>%
  tokens_remove(stopwords("en")) %>% 
  tokens_wordstem(language = "en") %>% 
  dfm() %>% 
  dfm_trim(min_docfreq = 0.005,
           max_docfreq = 0.900,
           docfreq_type = "prop") %>% 
  dfm_replace(featnames(OWNDFM_trim), 
              make.names(featnames(OWNDFM_trim), unique = TRUE)) %>% 
  dfm_tfidf(scheme_tf = "count")

OWNDFM_final

### create dfm
OWNDFM_final
```

### Machine Learning
```{r}
TEXTOWN = convert(OWNDFM_final, to = "data.frame")[,-1]
METADATAOWN = docvars(OWNDFM_final)
DEPVARNAME = ""
Xo = cbind(TEXTOWN , METADATAOWN) 
Xo = model.matrix(DEPVARNAME~., X)
set.seed(735)
trainOWN <- sample(1:nrow(R_DFM_final), 0.7*nrow(R_DFM_final))
length(trainOWN)
###
reddit_train_Xo <- Xo[train,]
  reddit_test_Xo <- Xo[-train,]
reddit_train_Yo = docvars(OWNDFM_final)$DEPVARNAME[train]
  reddit_test_Yo = docvars(OWNDFM_final)$DEPVARNAME[-train] 
```

```{r}
ctrl <- trainControl(method = "cv", number = 5,
                     classProbs = TRUE, 
                     summaryFunction = prSummary,
                     savePredictions = TRUE)

train_lasso1_OWN <- train(x = reddit_train_Xo, y = reddit_train_Yo, 
                     method = "glmnet",
                     metric = "F",
                     tuneGrid = tuneGrid_glm,
                     trControl = ctrl)

categ <- predict(train_lasso1_OWN, newdata = reddit_test_Xo)
confusionMatrix(categ, reddit_test_Yo, positive = "nameofyourclass", mode = "prec_recall")
```

